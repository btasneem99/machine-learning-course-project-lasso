---
title: "SDS/CSC 293 Mini-Project 5: LASSO"
author: "Group 12: Ann Mudanye, Bushra Tasneem, Olivia Baldwin"
date: "Thursday, May 2^nd^, 2019"
output:
  html_document:
    highlight: tango
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: true
    df_print: kable
---

```{r setup, include=FALSE}
# Load all your packages here:
library(tidyverse)
library(glmnet)
library(modelr)
library(broom)
library(skimr)
library(Metrics)
library(Hmisc)


# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 16/2, fig.height = 9/2
)

# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)

```

You will be submiting an entry to Kaggle's [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/){target="_blank"} by fitting a fitted ~~spline~~, ~~multiple regression~~ LASSO regularized multiple regression model $\hat{f}(x)$.

However of the original 1460 rows of the `training` data, in the `data/` folder you are given a `train.csv` consisting of only 50 of the rows!


***

# Load data

Read in data provided by Kaggle for this competition. They are organized in the `data/` folder of this RStudio project:

```{r}
training <- read_csv("data/train.csv") %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  ) %>% 
  # Fit your models to this outcome variable:
  mutate(log_SalePrice = log(SalePrice+1))

test <- read_csv("data/test.csv")%>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )
sample_submission <- read_csv("data/sample_submission.csv")

# Function that takes in a LASSO fit object and returns a "tidy" data frame of
# the beta-hat coefficients for each lambda value used in LASSO fit. 
get_LASSO_coefficients <- function(LASSO_fit){
  beta_hats <- LASSO_fit %>%
    broom::tidy(return_zeros = TRUE) %>%
    select(term, estimate, lambda) %>%
    arrange(desc(lambda))
  return(beta_hats)
}
```


## Look at your data!

Always, ALWAYS, **ALWAYS** start by looking at your raw data. This gives you visual sense of what information you have to help build your predictive models. To get a full description of each variable, read the data dictionary in the `data_description.txt` file in the `data/` folder.

Note that the following code chunk has `eval = FALSE` meaning "don't evaluate this chunk with knitting" because `.Rmd` files won't knit if they include a `View()`:

```{r, eval = FALSE}
View(training)
glimpse(training)

View(test)
glimpse(test)

# Pay close attention to the variables and variable types in sample_submission. 
# Your submission must match this exactly.
glimpse(sample_submission)

# Hint:
skim(training)
skim(test)

#imputed_Data <- mice(training,m=1,maxit=5,meth='pmm',seed=500)
#completedData <- complete(imputed_Data,1)

```

## Data wrangling

```{r}

#imputation of numerical variables for training and test data 
train <- training %>%
  mutate(
   MSSubClass = ifelse(is.na(MSSubClass), mean(MSSubClass, na.rm=TRUE), MSSubClass),
   LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm=TRUE), LotFrontage),
   LotArea = ifelse(is.na(LotArea), mean(LotArea, na.rm=TRUE), LotArea),
   OverallQual = ifelse(is.na(OverallQual), mean(OverallQual, na.rm=TRUE), OverallQual),
   OverallCond = ifelse(is.na(OverallCond), mean(OverallCond, na.rm=TRUE), OverallCond),
   YearBuilt = ifelse(is.na(YearBuilt), mean(YearBuilt, na.rm=TRUE), YearBuilt),
   YearRemodAdd = ifelse(is.na(YearRemodAdd), mean(YearRemodAdd, na.rm=TRUE), YearRemodAdd),
   MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm=TRUE), MasVnrArea),
   BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), mean(BsmtFinSF1, na.rm=TRUE), BsmtFinSF1),
   BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), mean(BsmtFinSF2, na.rm=TRUE), BsmtFinSF2),
   BsmtUnfSF = ifelse(is.na(BsmtUnfSF), mean(BsmtUnfSF, na.rm=TRUE), BsmtUnfSF),
   TotalBsmtSF = ifelse(is.na(TotalBsmtSF), mean(TotalBsmtSF, na.rm=TRUE), TotalBsmtSF),
   FirstFlrSF = ifelse(is.na(FirstFlrSF), mean(FirstFlrSF, na.rm=TRUE), FirstFlrSF),
   SecondFlrSF = ifelse(is.na(SecondFlrSF), mean(SecondFlrSF, na.rm=TRUE), SecondFlrSF),
   LowQualFinSF = ifelse(is.na(LowQualFinSF), mean(LowQualFinSF, na.rm=TRUE), LowQualFinSF),
   GrLivArea = ifelse(is.na(GrLivArea), mean(GrLivArea, na.rm=TRUE), GrLivArea),
   BsmtFullBath = ifelse(is.na(BsmtFullBath), mean(BsmtFullBath, na.rm=TRUE), BsmtFullBath),
   BsmtHalfBath = ifelse(is.na(BsmtHalfBath), mean(BsmtHalfBath, na.rm=TRUE), BsmtHalfBath),
   FullBath = ifelse(is.na(FullBath), mean(FullBath, na.rm=TRUE), FullBath), 
   HalfBath = ifelse(is.na(HalfBath), mean(HalfBath, na.rm=TRUE), HalfBath),
   BedroomAbvGr = ifelse(is.na(BedroomAbvGr), mean(BedroomAbvGr, na.rm=TRUE), BedroomAbvGr),
   KitchenAbvGr = ifelse(is.na(KitchenAbvGr), mean(KitchenAbvGr, na.rm=TRUE), KitchenAbvGr),
   TotRmsAbvGrd = ifelse(is.na(TotRmsAbvGrd), mean(TotRmsAbvGrd, na.rm=TRUE), TotRmsAbvGrd),
   Fireplaces = ifelse(is.na(Fireplaces), mean(Fireplaces, na.rm=TRUE), Fireplaces),
   GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm=TRUE), GarageYrBlt),
   GarageCars = ifelse(is.na(GarageCars), mean(GarageCars, na.rm=TRUE), GarageCars),
   GarageArea = ifelse(is.na(GarageArea), mean(GarageArea, na.rm=TRUE), GarageArea),
   WoodDeckSF = ifelse(is.na(WoodDeckSF), mean(WoodDeckSF, na.rm=TRUE), WoodDeckSF),
   OpenPorchSF = ifelse(is.na(OpenPorchSF), mean(OpenPorchSF, na.rm=TRUE), OpenPorchSF),
   EnclosedPorch = ifelse(is.na(EnclosedPorch), mean(EnclosedPorch, na.rm=TRUE), EnclosedPorch),
   ThirdSsnPorch = ifelse(is.na(ThirdSsnPorch), mean(ThirdSsnPorch, na.rm=TRUE), ThirdSsnPorch),
   ScreenPorch = ifelse(is.na(ScreenPorch), mean(ScreenPorch, na.rm=TRUE), ScreenPorch),
   PoolArea = ifelse(is.na(PoolArea), mean(PoolArea, na.rm=TRUE), PoolArea),
   MiscVal = ifelse(is.na(MiscVal), mean(MiscVal, na.rm=TRUE), MiscVal),
   MoSold = ifelse(is.na(MoSold), mean(MoSold, na.rm=TRUE), MoSold),
   YrSold = ifelse(is.na(YrSold), mean(YrSold, na.rm=TRUE), YrSold))


testing <- test %>%
  mutate(
   MSSubClass = ifelse(is.na(MSSubClass), mean(MSSubClass, na.rm=TRUE), MSSubClass),
   LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm=TRUE), LotFrontage),
   LotArea = ifelse(is.na(LotArea), mean(LotArea, na.rm=TRUE), LotArea),
   OverallQual = ifelse(is.na(OverallQual), mean(OverallQual, na.rm=TRUE), OverallQual),
   OverallCond = ifelse(is.na(OverallCond), mean(OverallCond, na.rm=TRUE), OverallCond),
   YearBuilt = ifelse(is.na(YearBuilt), mean(YearBuilt, na.rm=TRUE), YearBuilt),
   YearRemodAdd = ifelse(is.na(YearRemodAdd), mean(YearRemodAdd, na.rm=TRUE), YearRemodAdd),
   MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm=TRUE), MasVnrArea),
   BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), mean(BsmtFinSF1, na.rm=TRUE), BsmtFinSF1),
   BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), mean(BsmtFinSF2, na.rm=TRUE), BsmtFinSF2),
   BsmtUnfSF = ifelse(is.na(BsmtUnfSF), mean(BsmtUnfSF, na.rm=TRUE), BsmtUnfSF),
   TotalBsmtSF = ifelse(is.na(TotalBsmtSF), mean(TotalBsmtSF, na.rm=TRUE), TotalBsmtSF),
   FirstFlrSF = ifelse(is.na(FirstFlrSF), mean(FirstFlrSF, na.rm=TRUE), FirstFlrSF),
   SecondFlrSF = ifelse(is.na(SecondFlrSF), mean(SecondFlrSF, na.rm=TRUE), SecondFlrSF),
   LowQualFinSF = ifelse(is.na(LowQualFinSF), mean(LowQualFinSF, na.rm=TRUE), LowQualFinSF),
   GrLivArea = ifelse(is.na(GrLivArea), mean(GrLivArea, na.rm=TRUE), GrLivArea),
   BsmtFullBath = ifelse(is.na(BsmtFullBath), mean(BsmtFullBath, na.rm=TRUE), BsmtFullBath),
   BsmtHalfBath = ifelse(is.na(BsmtHalfBath), mean(BsmtHalfBath, na.rm=TRUE), BsmtHalfBath),
   FullBath = ifelse(is.na(FullBath), mean(FullBath, na.rm=TRUE), FullBath), 
   HalfBath = ifelse(is.na(HalfBath), mean(HalfBath, na.rm=TRUE), HalfBath),
   BedroomAbvGr = ifelse(is.na(BedroomAbvGr), mean(BedroomAbvGr, na.rm=TRUE), BedroomAbvGr),
   KitchenAbvGr = ifelse(is.na(KitchenAbvGr), mean(KitchenAbvGr, na.rm=TRUE), KitchenAbvGr),
   TotRmsAbvGrd = ifelse(is.na(TotRmsAbvGrd), mean(TotRmsAbvGrd, na.rm=TRUE), TotRmsAbvGrd),
   Fireplaces = ifelse(is.na(Fireplaces), mean(Fireplaces, na.rm=TRUE), Fireplaces),
   GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm=TRUE), GarageYrBlt),
   GarageCars = ifelse(is.na(GarageCars), mean(GarageCars, na.rm=TRUE), GarageCars),
   GarageArea = ifelse(is.na(GarageArea), mean(GarageArea, na.rm=TRUE), GarageArea),
   WoodDeckSF = ifelse(is.na(WoodDeckSF), mean(WoodDeckSF, na.rm=TRUE), WoodDeckSF),
   OpenPorchSF = ifelse(is.na(OpenPorchSF), mean(OpenPorchSF, na.rm=TRUE), OpenPorchSF),
   EnclosedPorch = ifelse(is.na(EnclosedPorch), mean(EnclosedPorch, na.rm=TRUE), EnclosedPorch),
   ThirdSsnPorch = ifelse(is.na(ThirdSsnPorch), mean(ThirdSsnPorch, na.rm=TRUE), ThirdSsnPorch),
   ScreenPorch = ifelse(is.na(ScreenPorch), mean(ScreenPorch, na.rm=TRUE), ScreenPorch),
   PoolArea = ifelse(is.na(PoolArea), mean(PoolArea, na.rm=TRUE), PoolArea),
   MiscVal = ifelse(is.na(MiscVal), mean(MiscVal, na.rm=TRUE), MiscVal),
   MoSold = ifelse(is.na(MoSold), mean(MoSold, na.rm=TRUE), MoSold),
   YrSold = ifelse(is.na(YrSold), mean(YrSold, na.rm=TRUE), YrSold))

test2 <- test %>%
  select( Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, FirstFlrSF, SecondFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, ThirdSsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold, RoofStyle, SaleType, SaleCondition)


```


***



# Minimally viable product

Since we have already performed exploratory data analyses of this data in MP1 and MP2, let's jump straight into the modeling. For this phase:

* Train an unregularized standard multiple regression model $\widehat{f}_1$ using **all** 36 numerical variables as predictors.


```{r}
# Train your model here:

# Model formula
model_formula <- "log_SalePrice ~ MSSubClass + LotFrontage + LotArea + 
OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + 
BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + FirstFlrSF + SecondFlrSF + LowQualFinSF + 
GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + 
KitchenAbvGr + TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + GarageArea + 
WoodDeckSF + OpenPorchSF + EnclosedPorch + ThirdSsnPorch + ScreenPorch + PoolArea + 
MiscVal + MoSold + YrSold" %>% 
  as.formula()

#fit model to train data 
fitted_model <- lm(model_formula, data = train)


```



***



# Due diligence

* Compute two RMLSE's of the fitted model $\widehat{f}_1$
      a) on the `training` data. You may use a function from a package to achieve this.
      b) on the `test` data via a submission to Kaggle `data/submit_regression.csv`.
* Compare the two RMLSE's. If they are different, comment on why they might be different.

```{r}

#compute RMLSE on train data : 

#Make predictions using train data 
predicted_points_1 <- fitted_model %>%
    broom::augment(newdata = train)

#Save predictions in train data frame
train <- train %>% 
    mutate( logSalePrice_hat = predicted_points_1$.fitted,
      SalePrice_hat = (exp(logSalePrice_hat)-1))

#calculate RMLSE 
rmsle(train$SalePrice, train$SalePrice_hat)


#compute RMLSE on test data : 

#Make predictions using testing data 
predicted_points_2 <- fitted_model %>%
    broom::augment(newdata = testing)

#Save predictions in testing data frame
testing <- testing %>% 
    mutate( logSalePrice_hat = predicted_points_2$.fitted,
      SalePrice_hat = (exp(logSalePrice_hat)-1))

submission <- sample_submission %>% 
  mutate(SalePrice = testing$SalePrice_hat)

write_csv(submission, path = "data/submission_due_diligence.csv")

```

![](duediligence_score.png){ width=100% }

RMLSE on training  | RMLSE on test (via Kaggle)
------------------ | -------------
0.0476             | 0.225


The RMLSE on the training data is 0.476 and the RMLSE on the test data (from Kaggle submission) is 0.225. The RMLSE on the test data is approximately 5 times bigger. Because our training data consist of only 50 observations, the model is overfitted to the test data. Hence, when we fit the model on the training data, we get an error which is much larger.

***



# Reaching for the stars

1. Find the $\lambda^*$ tuning parameter that yields the LASSO model with the
lowest estimated RMLSE as well as this lowest RMLSE as well. You may use functions included in a package for this.
1. Convince yourself with a visualization that the $\lambda^*$ you found is indeed the one that returns the lowest estimated RMLSE.
1. What is the model $\widehat{f}$_2 resulting from this $\lambda^*$? Output a data frame of the $\widehat{\beta}$.
1. Visualize the progression of $\widehat{\beta}$ for different $\lambda$ values and mark $\lambda^*$ with a vertical line:



```{r}

set.seed(76)

# Find lambda star:

#split data in 5 folds
train <- train %>% 
  sample_frac(1) %>% 
  mutate(fold = rep(1:5, length = n())) %>% 
  arrange(fold)

#create a table to keep track of lambda and RMLSE 
lambda_RMLSE <- tibble(
  lambda = 10^seq(from = -4, to = 3, length = 100),
  RMLSE = 0
)
  
for(i in 1:nrow(lambda_RMLSE)){
  lambda <- lambda_RMLSE$lambda[i]
  
  RMLSE <- rep(0, 5)
  for(j in 1:5){
    pretend_training <- train %>% 
      filter(fold != j)
    pretend_test <- train %>% 
      filter(fold == j) %>%
      select(-log_SalePrice)
    
    # model matrix representation of predictor variables for training set:
    x_matrix_train <- pretend_training %>%
      modelr::model_matrix(model_formula, data = .) %>%
      select(-`(Intercept)`) %>%
      as.matrix()
    
    # model matrix representation of predictor variables for test set:
    x_matrix_test <- pretend_test %>%
      mutate(log_SalePrice = 1) %>%
      modelr::model_matrix(model_formula, data = .) %>%
      select(-`(Intercept)`) %>%
      as.matrix()
    
    
    # Fit/train model to training 
    LASSO_fit_CV <- glmnet(x = x_matrix_train, y = pretend_training$log_SalePrice, alpha = 1, lambda = lambda)
    
    # Predict y_hat's for test data using model and same lambda.
    pretend_test <- pretend_test %>%
      mutate(logSalePrice_hat = predict(LASSO_fit_CV, newx = x_matrix_test, s = lambda)[,1],SalePrice_hat = (exp(logSalePrice_hat)-1) )


    
    RMLSE[j] <- pretend_test %>% 
      mutate(
        residual = log(SalePrice + 1) - log(SalePrice_hat + 1),
        residual_sq = residual^2
      ) %>% 
      dplyr::summarize(
        MLSE = mean(residual_sq),
        RMLSE = sqrt(MLSE)
      ) %>% 
      pull(RMLSE)
  }
  
  lambda_RMLSE$RMLSE[i] <- mean(RMLSE) 
}

#find lambda that returns the lowest rmlse 
lambda_star<- with(lambda_RMLSE, lambda[which.min(RMLSE)])
print(lambda_star)
print(min(lambda_RMLSE$RMLSE))

```

```{r}
# Create visualization here:

ggplot()+
    geom_line(data = lambda_RMLSE, aes(x = lambda, y = RMLSE), colour = 'blue', size = 1) + 
   scale_x_log10() +
  geom_point(data = lambda_RMLSE, aes(y=min(RMLSE), x=lambda_star), colour = 'black',size=3) +
  labs(title="Relationship between RMLSE and lambda")
```

```{r}
# Output data frame of beta-hats for the LASSO model that uses lambda_star:
train_matrix <- train %>%
      modelr::model_matrix(model_formula, data = .) %>%
      select(-`(Intercept)`) %>%
      as.matrix()

LASSO_fitted <- glmnet(x = train_matrix, y = train$log_SalePrice, alpha = 1, lambda = lambda_star)
get_LASSO_coefficients(LASSO_fitted) %>%
  spread(lambda, estimate)

```

```{r}
# Visualize the progression of beta-hats for different lambda values and mark lambda_star with a vertical line:

#create a range of lambda inputs 
lambda_inputs <- 10^seq(from = -4, to = 3, length = 100)

#fit model 
LASSO_fit <- glmnet(x = train_matrix, y = train$log_SalePrice, alpha = 1, lambda = lambda_inputs)


# Plot all beta-hats with lambda on log10-scale
LASSO_coefficients_plot <- get_LASSO_coefficients(LASSO_fit) %>%
  filter(term != "(Intercept)") %>%
  # Plot:
  ggplot(aes(x = lambda, y = estimate, col = term)) +
  geom_line() +
  labs(x = "lambda (log10-scale)", y = "beta-hat") +
  scale_x_log10()


LASSO_coefficients_plot +
  coord_cartesian(xlim = c(10^-4, 10^0), ylim = c(-0.35, 0.35)) +
  geom_vline(xintercept = lambda_star, linetype = "dashed", col = "blue")

```


***

# Point of diminishing returns

1. In qualitative language, comment on the resulting amount of shrinkage in the LASSO model?
1. Obtain the RMLSE of the fitted model
      a) on the `training` data
      b) on the `test` data via a submission to Kaggle `data/submit_LASSO.csv` that we will test.
1. Compare the two RMLSE's. If they are different, comment on why they might be different.

```{r}

#rmlse of training data 

#Predict y_hat's for train data using model and same lambda.
    train2 <- train %>%
      mutate(y_hat_LASSO = predict(LASSO_fitted, newx = train_matrix, s = lambda_star)[,1],SalePrice_hat = (exp(y_hat_LASSO)-1) )

rmsle(train$SalePrice, train2$SalePrice_hat)

test_matrix <- testing %>%
  # Create temporary outcome variance just to get model matrix to work:
  mutate(log_SalePrice = 1) %>%
  modelr::model_matrix(model_formula, data = .) %>%
  select(-`(Intercept)`) %>%
  as.matrix()

testing2 <- testing %>%
  mutate(y_hat_LASSO = predict(LASSO_fitted, newx = test_matrix, s = lambda_star)[,1], SalePrice_hat = (exp(y_hat_LASSO)-1))

submission <- sample_submission %>% 
  mutate(SalePrice = testing2$SalePrice_hat)

write_csv(submission, path = "data/submit_LASSO.csv")


```

![](lasso.png){ width=100% }

Comparing both RMLSE's here:

Method           | RMLSE on training  | RMLSE on test (via Kaggle)
---------------- | ------------------ | -------------
Unregularized lm | 0.0476             | 0.225
LASSO            | 0.0835             | 0.198


As lambda increases, the beta-hat goes towards zero because as LASSOâ€™s lambda coefficient increases, it increasingly regularizes the existing multiple regression model until all we are left with are the intercepts of the model for each predictor.

The RMLSE on training data for the LASSO model is larger (approximately 2x) than the RMLSE of the unregularized lm model on training data. The reason why this occurs is because LASSO has shrunk the beta coefficients of variables and has even dropped several variables from the model by shrinking them to 0. This means that compared to the unregularized lm model, the LASSO model is less overfitted. Because it reduces overfitting on the training data, the RMLSE on the training data is larger for the LASSO model.

The RMLSE on test data for the LASSO model is smaller than the RMLSE of the unregularizd lm model on test data. When we fit a LASSO model on the training data, we reduce overfitting. Hence, the predictions that we make on the test data are more accurate as compared to the predictions we make using the unregularized lm model. As the accuracy increases, the error of our predictions on test data decreases. Therefore, the RMLSE on test data when we use a LASSO model is smaller.


***


# Polishing the cannonball

1. Fit a LASSO model $\widehat{f}_3$ that uses categorical variables as well.
1. Output a `data/submit_LASSO_2.csv`
1. Submit to Kaggle and replace the screenshot below with an screenshot of your score.
1. Try to get the best Kaggle leaderboard score!

```{r,eval=FALSE}
# Model formula with added categorical variables 
#Attempt at cannonball 

model_formula_2 <- "log_SalePrice ~ MSSubClass + LotFrontage + LotArea + 
OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + 
BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + FirstFlrSF + SecondFlrSF + LowQualFinSF + 
GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + 
KitchenAbvGr + TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + GarageArea + 
WoodDeckSF + OpenPorchSF + EnclosedPorch + ThirdSsnPorch + ScreenPorch + PoolArea + 
MiscVal + MoSold + YrSold+ RoofStyle + SaleType+ SaleCondition" %>% 
  as.formula()


train_matrix_2 <- train %>%
      modelr::model_matrix(model_formula_2, data = .) %>%
      select(-`(Intercept)`) %>%
      as.matrix()

test_matrix_2 <- test2 %>%
      mutate(log_SalePrice = 1) %>%
      modelr::model_matrix(model_formula_2, data = .) %>%
      select(-`(Intercept)`) %>%
      as.matrix()
  
#find best lambda
LASSO_CV <- cv.glmnet(
  x = train_matrix_2,
  y = train$log_SalePrice,
  alpha = 1,
  lambda = lambda_inputs,
  nfolds = 5,
  type.measure = "mse"
)
LASSO_CV %>%
  broom::tidy() %>%
  rename(mse = estimate) %>%
  arrange(mse)

lambda_star2 <- LASSO_CV$lambda.min
lambda_star2


#fit model 
LASSO_fit_final <- glmnet(x = train_matrix_2, y = train$log_SalePrice, alpha = 1, lambda = lambda_star2)

#error because of wrong dimensions
testing3 <- test2 %>%
  mutate(y_hat_LASSO = predict(LASSO_fit_final, newx = test_matrix_2, s = lambda_star2)[,1], SalePrice_hat = (exp(y_hat_LASSO)-1))

submission <- sample_submission %>% 
  mutate(SalePrice = testing3$SalePrice_hat)

write_csv(submission, path = "data/submit_LASSO_2.csv")



```


![](score_screenshot.png){ width=100% }





